# HabermanMLAnalysis
Machine Learning and Data Analysis for Haberman Cancer Survival Dataset

In my final project, I had a goal of analyzing the Haberman dataset, sourced from the UCI Machine Learning Repository, with hopes of predicting the survival status of patients undergoing surgery for breast cancer. I had an end goal of creating a user-interactive program where anyone running the program can input their own data to predict survival outcomes post-breast cancer surgery. The program asks for the patient’s age, the year the surgery is performed, and the number of cancerous nodes present, and predicts whether the patient will survive more than 5 years after the surgery. In addition, I hoped to better visualize the correlations between each of these features and survival, as well as evaluate the machine learning models I created to do my predictions. 

My program starts by fetching the Haberman dataset from the UCI Machine Learning Repository and processing it, changing column values like 'Year' to represent the full year and changing the 'Status' column into binary format to indicate survival for more than 5 years after surgery or death within 5 years of surgery (survival = 0, death = 1).

The program uses many different computational techniques, including machine learning model training, prediction, and evaluation. After preprocessing, the data is split into training and testing sets. Then, two machine learning models are trained: logistic regression and K-nearest neighbors (KNN). Both models are fitted using the training data to learn patterns in the dataset. After training, the program asks the user to input information about a patient, such as age, year of operation, and number of positive cancerous axillary nodes detected, to predict their survival status using both models. The program then uses computation again to calculate the accuracy of each model's predictions and compares it to flipping a coin, providing insights into how effective each model is.

The program then employs statistical analysis to understand the dataset more. I use the code to calculate point-biserial correlation coefficients between features (age, nodes, year) and the target variable (survival). I also performed chi-squared tests for independence to determine relationships between each categorical variable and survival status to see the patterns within the dataset. 

I also used data visualization throughout my program to present the results of my machine learning model evaluation and statistical analysis tests in a visual format. The program uses both matplotlib and seaborn to create visualizations. For each feature in which a machine learning model is created, I created a heatmap confusion matrix, calibration curve, and ROC curve. I also created bar plots to cross validation for each model. In addition, I created bar plots for the point-biserial correlation coefficients and chi-squared test statistics.

The first function in the program is fetch_haberman(), which acts as the path to obtain the Haberman dataset from the UCI Machine Learning Repository, making sure the program has the necessary data to proceed. After this, the next function is loadData(), which works to load and preprocess the dataset, making sure it's formatted correctly for further analysis. The next function, processData(dataSet) refines the dataset, changing column values to make sure everything is consistent. It makes sure the data under 'Year' is in full years, and ressigns survival mappings into a binary format, where 0 = survival for more than 5 years after surgery, and death within 5 years of surgery = 1.

My next function, splitData(dataframe, testSize, randState) divides the dataset into training and testing sets, which is needed for model training. Next, I have my functions logRegression(xTrain, yTrain) and KNN(xTrain, yTrain, nNeighbors) which trains logistic regression and K-Nearest Neighbors (KNN) models respectively to predict survival status based on the dataset's features.

  My next function helps facilitate user interaction, called getUserInput(age_range, year_range, nodes_range), which asks users to input data for prediction. This input is then processed and used for predictions through my next function: makePrediction(model, inputData), which provides users with insights into survival likelihood given the inputs they provided.
  
Statistical analyses are visualized by visualiseStats(all_correlationCoeffs, all_chiSqResults), which gives information on feature-survival relationships with correlation coefficients and chi-squared test results. My next function, crossValidateMLModel(model, X, y, runs, feature_name, model_name) performs cross-validation to assess model performance beyond training data, with visualizeCrossValid(cvResults, feature_name, model_name) which gives insights into model consistency and efficacy across different folds.

Lastly, my visualizePerformance(model, xTest, yTest, feature_name, model_name) function gives a broad view of model performance, including confusion matrices, calibration curves, and ROC curves. All these functions are called in main(), starting from data loading to model evaluation and visualization, allowing for this analysis of the Haberman dataset and utilizing its predictions for survival status.

  I made quite a few simplifications when making my project. The main one is that survival predictions are very complex, and rely on many more factors than I analyzed in my project. To begin, my predictions utilized just information on the year of the surgery, age, and number of cancerous nodes. However, the impact of each of these factors may change over time. Advancements in medical technology and treatment protocols may improve survival rates over the years. Therefore, models trained on historical data may not accurately capture these temporal changes. My model is trained on years from 1958 to 1969, and surgery was much different then. If we are predicting surgery done in 2024 and beyond, the model might be able to do as well with predictions due to all of the medical advancements that have been made since 1969. Predicting survival for cases outside the dataset might be generalizing too much. This extrapolation, especially those with characteristics significantly different from the training data (especially for patients under 30 years old, years past 1969), may lead to unreliable predictions. Lastly, the dataset may not capture all relevant factors influencing survival after surgery. For instance, genetic predispositions, socio-economic status, and access to healthcare could significantly impact survival outcomes but may not be included in the dataset and therefore not included in the model.
  
During the development of my project, one issue I encountered was the need for better visualization of model performance. Initially, I had planned to focus mainly on confusion matrices for evaluation. To add better visualization, I decided to also create ROC curves and calibration curves to provide a more advanced visualization of my machine learning model performance. In addition, I added an originally unplanned user input prediction feature that allows users to input their data and obtain survival predictions, enhancing the project's interactivity and practical utility.

Moving forward, there are many different ways I can expand on this project. First, analyzing additional features beyond age, year of operation, and number of nodes could provide a more comprehensive understanding of survival after surgery. For example, incorporating genetic markers, previous medical history, and treatment details could offer important insights into survival outcomes. In addition, exploring more advanced machine learning techniques, such as ensemble methods or deep learning, could improve prediction accuracy and capture complex relationships within the data. Integrating real-time data sources and continuous model updating could ensure the model remains relevant and effective in a dynamic healthcare environment. 

The majority of the results from my machine learning model analysis is displayed in the output once you run the main function. For each feature, I trained a machine learning model using logistic regression and k-Nearest Neighbors, and calculated various statistics for model performance, including accuracy, precision, recall, F1 score, and ROC AUC. Accuracy measures the proportion of correctly classified instances ( true positives and true negatives) out of the total instances. Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It indicates the model's ability to avoid false positives. Recall measures the proportion of true positive predictions out of all actual positive instances in the dataset. It assesses the model's ability to capture all positive instances. The F1 score is the average of precision and recall. It provides a useful balance between precision and recall. The Receiver Operating Characteristic Area Under the Curve (ROC AUC) measures the area under the ROC curve, which plots the true positive rate against the false positive rate. A higher ROC AUC indicates better model performance.

Note: the 0.0 values in precision, recall, and F1 score are results of the imbalanced dataset I trained my model with. There's a significant class imbalance in the dataset, with the number of instances labeled as "Survived" far outnumbering those labeled as "Not Survived." This class imbalance could be a contributing factor to the low precision, recall, and F1 score for the minority class

To run my code, simply run the main function and follow the user input instructions. They should be clear, but if you want to predict within the dataset (not extrapolate past the dataset’s range of ages, years, and nodes), type within, then values that are within the given range. Otherwise, type outside, then any values you wish. To test my code, simply run each code cell in the unit tests code file. 

