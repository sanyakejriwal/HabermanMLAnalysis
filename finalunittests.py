# -*- coding: utf-8 -*-
"""FinalUnitTests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mMxmtays14sVLz2Mf0GGMKVFWQvivyan
"""

pip install pandas scikit-learn matplotlib seaborn scipy requests

pip install --upgrade scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,
    roc_curve, roc_auc_score, classification_report
)
from scipy.stats import pointbiserialr, chi2_contingency
from sklearn.calibration import calibration_curve
from sklearn.model_selection import train_test_split, cross_val_score
import requests
from io import StringIO

from unittest.mock import patch
import unittest

def fetch_haberman():
    """ Fetches the Haberman dataset from the UCI Machine Learning Repository and returns it as a DataFrame. """
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data"
    response = requests.get(url)
    data = StringIO(response.text)
    column_names = ['Age', 'Year', 'Nodes', 'Status']
    df = pd.read_csv(data, names=column_names)
    return df

# Unit test for the fetch_haberman function
class TestFetchHaberman(unittest.TestCase):
    def test_fetch_haberman(self):
        # Prepare mock data and response
        mock_data = """30,64,1,1
                       30,62,3,1
                       30,65,0,1"""
        expected_df = pd.read_csv(StringIO(mock_data), names=['Age', 'Year', 'Nodes', 'Status'])

        # Use patch to mock requests.get
        with patch('requests.get') as mock_get:
            mock_get.return_value.text = mock_data

            # Call the function
            result_df = fetch_haberman()

            # Assertions to check if the fetched data is as expected
            pd.testing.assert_frame_equal(result_df, expected_df)

# Run the tests
unittest.main(argv=[''], exit=False)

# Function to load and preprocess the data
def loadData():
    """ Loads and preprocesses the Haberman dataset. Returns: DataFrame, a Preprocessed DataFrame. """
    return fetch_haberman()

# Function to preprocess the data
def processData(dataSet):
    """ Preprocesses the dataset by adjusting column values. """
    dataSet['Year'] = dataSet['Year'].apply(lambda x: 1900 + x)
    dataSet['Status'] = dataSet['Status'].apply(lambda x: 0 if x == 1 else 1)  # Switching survival/death mapping
    return dataSet
# Unit test for loadData and processData functions
class TestHabermanDataFunctions(unittest.TestCase):
    def test_loadData(self):
        # Mocking fetch_haberman instead of requests.get directly
        with patch('__main__.fetch_haberman', return_value=pd.DataFrame({'Year': [70]})) as mock_fetch:
            df = loadData()
            mock_fetch.assert_called_once()  # Ensure fetch_haberman is called
            self.assertIsInstance(df, pd.DataFrame)  # Check if loadData returns a DataFrame

    def test_processData(self):
        # Create a sample DataFrame
        test_data = pd.DataFrame({
            'Year': [70, 68],
            'Status': [1, 2]
        })
        expected_data = pd.DataFrame({
            'Year': [1970, 1968],
            'Status': [0, 1]
        })

        # Process the data
        processed_data = processData(test_data)

        # Check if processed data matches expected outcomes
        pd.testing.assert_frame_equal(processed_data, expected_data)

# Run the tests
unittest.main(argv=[''], exit=False)

# Function to split the data into training and testing sets
def splitData(dataframe, testSize, randState):
    """ Splits the dataset into training and testing sets. Returns a tuple: Four DataFrames representing X_train, X_test, y_train, and y_test. """
    xData = dataframe.drop('Status', axis=1)
    yData = dataframe['Status']
    xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size=testSize, random_state=randState)
    return xTrain, xTest, yTrain, yTest

# Function to make predictions using a trained model
def predict(model, xTest):
    """ Predicts target labels using a trained model. Returns an array of Predicted labels. """
    yPrediction = model.predict(xTest)
    return yPrediction

# Unit test for splitData and predict functions
class TestDataFunctions(unittest.TestCase):
    def test_splitData(self):
        # Create a sample DataFrame
        df = pd.DataFrame({
            'Age': [30, 31, 32],
            'Year': [65, 66, 67],
            'Nodes': [1, 2, 3],
            'Status': [1, 0, 1]
        })
        # Split the data
        xTrain, xTest, yTrain, yTest = splitData(df, testSize=0.5, randState=42)

        # Check if the split is correct
        self.assertEqual(len(xTrain), len(yTrain))  # Length of training X and Y should match
        self.assertEqual(len(xTest), len(yTest))    # Length of testing X and Y should match
        self.assertEqual(len(xTrain) + len(xTest), len(df))  # Combined lengths should match original

    def test_predict(self):
        # Create a sample model and data
        model = LogisticRegression()
        xTrain = pd.DataFrame({'Age': [20, 21], 'Year': [70, 71], 'Nodes': [1, 0]})
        yTrain = pd.Series([0, 1])
        xTest = pd.DataFrame({'Age': [22], 'Year': [72], 'Nodes': [1]})

        # Train model
        model.fit(xTrain, yTrain)

        # Mock the model predict call
        with patch.object(model, 'predict', return_value=np.array([1])) as mock_predict:
            prediction = predict(model, xTest)
            mock_predict.assert_called_once_with(xTest)
            np.testing.assert_array_equal(prediction, np.array([1]))  # Check if prediction matches expected

# Run the tests
unittest.main(argv=[''], exit=False)

# Function to evaluate model performance
def evalPerformance(yTest, yPred, yProba):
    report = classification_report(yTest, yPred, target_names=['Survived', 'Not Survived'], output_dict=True, zero_division=0)
    fpr, tpr, thresholds = roc_curve(yTest, yProba[:, 1])
    auc = roc_auc_score(yTest, yProba[:, 1])
    metrics = {
        'Accuracy': accuracy_score(yTest, yPred),
        'Precision': precision_score(yTest, yPred, zero_division=0),
        'Recall': recall_score(yTest, yPred, zero_division=0),
        'F1 Score': f1_score(yTest, yPred, zero_division=0),
        'ROC AUC': auc
    }
    return metrics, report, (fpr, tpr, auc)

# Function to calculate correlations
def calculateCorrelations(df):
    correlationCoeff = {}
    for column in df.columns.drop('Status'):
        correlation, _ = pointbiserialr(df[column], df['Status'])
        correlationCoeff[column] = correlation
    return correlationCoeff

# Function to perform chi-squared tests
def chiSquaredTest(df):
    chiSqResults = {}
    for column in df.columns.drop('Status'):
        contingency_table = pd.crosstab(df[column], df['Status'])
        chi2, p_value, dof, expected = chi2_contingency(contingency_table)
        chiSqResults[column] = {'Chi-square statistic': chi2, 'p-value': p_value, 'Degrees of freedom': dof, 'Expected frequencies': expected}
    return chiSqResults

# Unit test for evalPerformance, calculateCorrelations, and chiSquaredTest functions
class TestDataAnalysisFunctions(unittest.TestCase):
    def test_evalPerformance(self):
        yTest = np.array([1, 0, 1, 0, 1])
        yPred = np.array([1, 0, 0, 0, 1])
        yProba = np.array([[0.2, 0.8], [0.7, 0.3], [0.65, 0.35], [0.9, 0.1], [0.3, 0.7]])
        metrics, report, roc_data = evalPerformance(yTest, yPred, yProba)
        self.assertEqual(metrics['Accuracy'], accuracy_score(yTest, yPred))  # Check accuracy
        self.assertIsInstance(report, dict)  # Check report is dictionary
        self.assertEqual(len(roc_data), 3)  # Check ROC data format

    def test_calculateCorrelations(self):
        df = pd.DataFrame({
            'Age': [30, 35, 40, 45, 50],
            'Year': [65, 66, 67, 68, 69],
            'Status': [1, 0, 1, 0, 1]
        })
        correlations = calculateCorrelations(df)
        self.assertTrue('Age' in correlations)  # Check if 'Age' correlation is calculated

    def test_chiSquaredTest(self):
        df = pd.DataFrame({
            'Age': [30, 35, 40, 45, 50],
            'Year': [65, 66, 67, 68, 69],
            'Status': [1, 0, 1, 0, 1]
        })
        chiSqResults = chiSquaredTest(df)
        self.assertTrue('Age' in chiSqResults)  # Check if 'Age' Chi-square results are included

# Run the tests
unittest.main(argv=[''], exit=False)

# Function to get user input for prediction
def getUserInput(age_range, year_range, nodes_range):
    """ Gets user input for prediction. Returns: a DataFrame of User input data. """
    while True:
        print("Would you like to predict within or outside the dataset? Predicting outside may be less accurate.")
        user_choice = input("Type 'within' or 'outside': ").strip().lower()
        if user_choice in ["within", "outside"]:
            break
        else:
            print("Invalid input! Please enter either 'within' or 'outside'.")

    if user_choice == "within":
        while True:
            age = int(input(f"Enter patient's age (range in dataset is {age_range[0]} to {age_range[1]}): "))
            if age_range[0] <= age <= age_range[1]:
                break
            print("Invalid age! Please enter a value within the specified range.")

        while True:
            year = int(input(f"Enter the year of operation (full year, range in dataset is {year_range[0]} to {year_range[1]}): "))
            if year_range[0] <= year <= year_range[1]:
                break
            print("Invalid year! Please enter a value within the specified range.")

        while True:
            nodes = int(input(f"Enter the number of positive axillary nodes detected (range in dataset is {nodes_range[0]} to {nodes_range[1]}): "))
            if nodes_range[0] <= nodes <= nodes_range[1]:
                break
            print("Invalid node count! Please enter a value within the specified range.")
    else:
        age = int(input("Enter patient's age: "))
        year = int(input("Enter the year of operation (full year): "))
        nodes = int(input("Enter the number of positive axillary nodes detected: "))

    return pd.DataFrame([[age, year, nodes]], columns=['Age', 'Year', 'Nodes'])

# Function to make predictions using a trained model
def makePrediction(model, inputData):
    """ Makes predictions using a trained model. Returns an array of Predicted labels. """
    prediction = model.predict(inputData)
    return prediction

# Function to visualize statistical analyses
def visualiseStats(all_correlationCoeffs, all_chiSqResults):
    """ Visualizes statistical analyses. """
    fig, ax = plt.subplots(1, 2, figsize=(15, 6))  # Modified to have one row and two columns

    # Collect all correlation coefficients and chi-squared statistics
    correlations = {feature: values for feature_dict in all_correlationCoeffs for feature, values in feature_dict.items()}
    chi2_stats = {feature: values for feature_dict in all_chiSqResults for feature, values in feature_dict.items()}

    # Bar plot for Point-Biserial Correlation Coefficients
    ax[0].bar(correlations.keys(), correlations.values(), color='dodgerblue')
    ax[0].set_title('Point-Biserial Correlation Coefficients')
    ax[0].set_ylabel('Correlation Coefficient')
    ax[0].set_xlabel('Variables')
    ax[0].set_xticks(range(len(correlations.keys())))  # Set tick positions
    ax[0].set_xticklabels(correlations.keys(), rotation=45, ha='right')  # Set tick labels

    # Bar plot for Chi-squared Test Statistics with p-values as labels
    bars = ax[1].bar(chi2_stats.keys(), [values['Chi-square statistic'] for values in chi2_stats.values()], color='salmon')
    ax[1].set_title('Chi-squared Test Statistics')
    ax[1].set_ylabel('Chi-square Statistic')
    ax[1].set_xlabel('Variables')
    ax[1].set_xticks(range(len(chi2_stats.keys())))  # Set tick positions
    ax[1].set_xticklabels(chi2_stats.keys(), rotation=45, ha='right')  # Set tick labels

    # Adding p-values as labels to each bar
    for bar, p_value in zip(bars, [values['p-value'] for values in chi2_stats.values()]):
        height = bar.get_height()
        ax[1].text(bar.get_x() + bar.get_width() / 2, height, f'p={p_value:.3f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()



# Functions are assumed to be defined above this point

# Testing the functions
class TestUserInteractions(unittest.TestCase):
    @patch('builtins.input', side_effect=['within', '30', '1970', '1'])  # Mocking input for 'within' scenario
    def test_getUserInput_within(self, mock_input):
        age_range = (20, 50)
        year_range = (1960, 1980)
        nodes_range = (0, 5)
        df = getUserInput(age_range, year_range, nodes_range)
        self.assertEqual(df.iloc[0]['Age'], 30)  # Assert the correct age was input and processed

    @patch('builtins.input', side_effect=['outside', '60', '1990', '10'])  # Mocking input for 'outside' scenario
    def test_getUserInput_outside(self, mock_input):
        age_range = (20, 50)
        year_range = (1960, 1980)
        nodes_range = (0, 5)
        df = getUserInput(age_range, year_range, nodes_range)
        self.assertEqual(df.iloc[0]['Age'], 60)  # Assert the correct age was input and processed

    @patch('builtins.input', side_effect=['invalid', 'within', '30', '1970', '1'])  # Handling invalid then correct input
    def test_getUserInput_invalid_then_within(self, mock_input):
        age_range = (20, 50)
        year_range = (1960, 1980)
        nodes_range = (0, 5)
        df = getUserInput(age_range, year_range, nodes_range)
        self.assertEqual(df.iloc[0]['Year'], 1970)  # Assert the correct year was input and processed

class TestModelPredictions(unittest.TestCase):
    def test_makePrediction(self):
        model = LogisticRegression()
        xTrain = pd.DataFrame({'Age': [20, 21], 'Year': [70, 71], 'Nodes': [1, 0]})
        yTrain = pd.Series([0, 1])
        model.fit(xTrain, yTrain)
        inputData = pd.DataFrame({'Age': [22], 'Year': [72], 'Nodes': [1]})
        prediction = makePrediction(model, inputData)
        self.assertIn(prediction[0], [0, 1])  # Assert that prediction is within expected output classes

class TestVisualization(unittest.TestCase):
    @patch('matplotlib.pyplot.show')  # Mock plt.show() to test visualization without rendering graphs
    def test_visualiseStats(self, mock_show):
        all_correlationCoeffs = [{'Age': 0.1, 'Year': -0.2}]
        all_chiSqResults = [{'Age': {'Chi-square statistic': 2.5, 'p-value': 0.1, 'Degrees of freedom': 1, 'Expected frequencies': np.array([10, 20])}}]
        visualiseStats(all_correlationCoeffs, all_chiSqResults)
        mock_show.assert_called_once()  # Check if plt.show() was called, indicating that the plot was attempted

# Run the tests
unittest.main(argv=[''], exit=False)

# Function to perform cross-validation for machine learning models
def crossValidateMLModel(model, X, y, runs, feature_name, model_name):
    """ Performs cross-validation for machine learning models. Returns a dictionary of cross-validation results. """
    scores = cross_val_score(model, X, y, cv=runs, scoring='accuracy')
    mean_score = np.mean(scores)
    std_dev_score = np.std(scores)
    return {'Fold Scores': scores, f'Mean Accuracy ({feature_name}, {model_name})': mean_score, f'Standard Deviation ({feature_name}, {model_name})': std_dev_score}

# Function to visualize cross-validation results
def visualizeCrossValid(cvResults, feature_name, model_name):
    """ Visualizes cross-validation results. """
    scores = cvResults['Fold Scores']
    mean_score = cvResults[f'Mean Accuracy ({feature_name}, {model_name})']
    std_dev_score = cvResults[f'Standard Deviation ({feature_name}, {model_name})']

    fig, ax = plt.subplots(figsize=(15, 6))  # Adjusted to match the size of other plots

    ax.bar(range(1, len(scores) + 1), scores, color='skyblue', label='Fold Score')
    ax.axhline(y=mean_score, color='r', linestyle='--', label=f'Mean Accuracy')
    ax.fill_between(range(1, len(scores) + 1), mean_score - std_dev_score, mean_score + std_dev_score, color='gray', alpha=0.2, label=f'Â±1 std dev')
    ax.set_title(f'Cross-Validation Consistency of Accuracy for {model_name} - {feature_name}')
    ax.set_xlabel('Fold Number')
    ax.set_ylabel('Accuracy')
    ax.set_xticks(range(1, len(scores) + 1))
    ax.legend()

    plt.show()

# Function to train logistic regression model
def logRegression(xTrain, yTrain):
    """ Trains a logistic regression model. Returns: LogisticRegression: Trained logistic regression model. """
    logReg = LogisticRegression(random_state=29, max_iter=1000)
    return logReg.fit(xTrain, yTrain)


class TestMLModelFunctions(unittest.TestCase):
    @patch('__main__.cross_val_score', return_value=np.array([0.8, 0.85, 0.82]))
    def test_crossValidateMLModel(self, mock_cross_val_score):
        model = LogisticRegression()
        X, y = np.random.rand(100, 10), np.random.randint(0, 2, 100)
        results = crossValidateMLModel(model, X, y, 3, 'Age', 'Logistic Regression')
        self.assertEqual(len(results['Fold Scores']), 3)  # Ensure we received three fold scores
        self.assertAlmostEqual(results['Mean Accuracy (Age, Logistic Regression)'], (0.8 + 0.85 + 0.82) / 3, places=5)
        self.assertIn('Standard Deviation (Age, Logistic Regression)', results)

    @patch('matplotlib.pyplot.show')
    def test_visualizeCrossValid(self, mock_show):
        cvResults = {
            'Fold Scores': np.array([0.8, 0.85, 0.82]),
            'Mean Accuracy (Age, Logistic Regression)': 0.823333333,
            'Standard Deviation (Age, Logistic Regression)': 0.02
        }
        visualizeCrossValid(cvResults, 'Age', 'Logistic Regression')
        mock_show.assert_called_once()

    def test_logRegression(self):
        X, y = np.array([[1, 2], [2, 3]]), np.array([0, 1])
        model = logRegression(X, y)
        self.assertIsInstance(model, LogisticRegression)
        self.assertTrue(hasattr(model, 'coef_'))

unittest.main(argv=[''], exit=False)

# Function to train KNN model
def KNN(xTrain, yTrain, nNeighbors):
    """ Trains a KNN classifier. Returns: KNeighborsClassifier: Trained KNN model."""
    neigh = KNeighborsClassifier(n_neighbors=nNeighbors)
    return neigh.fit(xTrain, yTrain)

# Function to visualize model performance
def visualizePerformance(model, xTest, yTest, feature_name, model_name):
    """ Visualizes model performance. Returns: dict: Performance metrics. """
    # Ensure we use only the columns that were used during model training
    xTest_subset = xTest[[feature_name]]
    yPred = model.predict(xTest_subset)
    yProba = model.predict_proba(xTest_subset)

    # Evaluate performance
    cm = confusion_matrix(yTest, yPred)
    accuracy = accuracy_score(yTest, yPred)
    precision = precision_score(yTest, yPred, zero_division=0)
    recall = recall_score(yTest, yPred, zero_division=0)
    f1 = f1_score(yTest, yPred, zero_division=0)

    # Adjust the size of the entire plot to accommodate three subplots
    fig, ax = plt.subplots(1, 3, figsize=(21, 6))

    # Plot Confusion Matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[0])
    ax[0].set_title(f'Confusion Matrix: {model_name} - {feature_name}')
    ax[0].set_xlabel('Predicted Labels')
    ax[0].set_ylabel('True Labels')
    ax[0].set_xticklabels(['Survived', 'Not Survived'])
    ax[0].set_yticklabels(['Survived', 'Not Survived'])

    # Plot Calibration Curve
    true_prob, pred_prob = calibration_curve(yTest, yProba[:, 1], n_bins=10)
    ax[1].plot(pred_prob, true_prob, marker='o', linestyle='-')
    ax[1].plot([0, 1], [0, 1], linestyle='--', color='gray')
    ax[1].set_xlabel('Mean Predicted Probability')
    ax[1].set_ylabel('Fraction of Positives')
    ax[1].set_title(f'Calibration Curve: {model_name} - {feature_name}')
    ax[1].grid(True)

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(yTest, yProba[:, 1])
    auc = roc_auc_score(yTest, yProba[:, 1])
    ax[2].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.2f})')
    ax[2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax[2].set_xlabel('False Positive Rate')
    ax[2].set_ylabel('True Positive Rate')
    ax[2].set_title(f'ROC Curve: {model_name} - {feature_name}')
    ax[2].legend(loc="lower right")
    ax[2].grid(True)

    plt.tight_layout()
    plt.show()

    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1, 'roc_auc': auc}


# Assuming the function definitions provided earlier for KNN and visualizePerformance

class TestKNNModel(unittest.TestCase):
    def test_KNN(self):
        xTrain = pd.DataFrame({
            'feature': [1, 2, 3, 4, 5],
            'another_feature': [5, 4, 3, 2, 1]
        })
        yTrain = np.array([0, 1, 0, 1, 0])
        model = KNN(xTrain, yTrain, 3)
        self.assertIsInstance(model, KNeighborsClassifier)

    @patch('matplotlib.pyplot.show')
    def test_visualizePerformance(self, mock_show):
        model = KNeighborsClassifier(n_neighbors=3)
        xTest = pd.DataFrame({
            'feature': [1, 2, 3]
        })
        yTest = np.array([0, 1, 0])
        model.fit(xTest, yTest)  # Fit model using the same feature subset as will be used for prediction
        results = visualizePerformance(model, xTest, yTest, 'feature', 'KNN')
        expected_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
        self.assertTrue(all(key in results for key in expected_metrics))
        self.assertTrue(mock_show.called)

unittest.main(argv=[''], exit=False)